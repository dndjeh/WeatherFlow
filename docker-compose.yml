services:
  zookeeper:
    image: wurstmeister/zookeeper:latest
    container_name: zookeeper
    ports:
      - "2181:2181"

  kafka:
    image: wurstmeister/kafka:latest
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - zookeeper

  producer:
    build: .
    container_name: producer
    depends_on:
      - kafka
    environment:
      API_KEY: ${API_KEY}
    volumes:
      - ./producer.py:/app/producer.py   # 로컬 producer.py → 컨테이너 /app/producer.py
    working_dir: /app
    command: python3 producer.py

  consumer:
    build: .
    container_name: consumer
    depends_on:
      - kafka
    volumes:
      - ./consumer.py:/app/consumer.py   # 로컬 consumer.py → 컨테이너 /app/consumer.py
    working_dir: /app
    command: python3 consumer.py
  
  preprocessing:
    image: jupyter/pyspark-notebook:latest
    container_name: preprocessing
    depends_on:
      - mysql
    volumes:
      - .:/app
    working_dir: /app
    command:  bash -c "pip install --no-cache-dir pyspark==3.5.5 && python preprocessing.py"
    restart: unless-stopped


  spark-master:
    image: bitnami/spark:3.5.5
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8080:8080"   # Spark Web UI
      - "7077:7077"   # Spark Master

  spark-worker:
    image: bitnami/spark:3.5.5
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    depends_on:
      - spark-master

  jupyter:
    image: jupyter/pyspark-notebook
    container_name: spark-jupyter
    ports:
      - "8888:8888"
    volumes:
      - ./notebooks:/home/jovyan/work
    depends_on:
      - spark-master
      - spark-worker

  mysql:
    image: mysql:8.0
    container_name: mysql
    ports:
      - "3307:3306"          # 로컬 3306 → 컨테이너 3306
    environment:
      MYSQL_ROOT_PASSWORD: rootpass
      MYSQL_DATABASE: toy_project
      MYSQL_USER: user
      MYSQL_PASSWORD: userpass
    volumes:
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    depends_on:
      - consumer
    # volumes:
    #   - ./mysql-data:/var/lib/mysql

  redis:
    image: redis:7.2.0
    container_name: redis
    ports:
      - "6379:6379"
    #command: python3 redis.py
    restart: unless-stopped

  redis-client:
    image: python:3.11-slim
    container_name: redis-client
    depends_on:
      - redis
    volumes:
      - .:/app
    working_dir: /app
    command: bash -c "pip install redis && python redis_test.py"


  airflow:
    build:
      context: .
      dockerfile: Dockerfile.airflow   # airflow 전용 Dockerfile
    #image: apache/airflow:2.8.1
    container_name: airflow
    restart: unless-stopped

    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__FERNET_KEY: '5ozw1xClhNUKElcA31BOLTCr3f8de2whokz13dLXQ1w='
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: 'mysql+pymysql://user:userpass@mysql:3306/airflow_db'
      AIRFLOW__WEBSERVER__RBAC: 'true'
    # env_file:
    #   - .env
    depends_on:
      - mysql
    ports:
      - "8081:8080"   # Airflow Web UI
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./requirements.txt:/requirements.txt
    command: bash -c "airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com && exec airflow webserver"

    #bash -c "pip install --no-cache-dir pymysql && airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com && exec airflow webserver"
    #bash -c "pip install --no-cache-dir -r /requirements.txt && exec airflow webserver"
  
  subway-api:
    build: .
    container_name: transit
    depends_on:
      - kafka        # Kafka 연동이 필요하면
    environment:
      SEOUL_SUBWAY_ARRIVAL_API_KEY: ${SEOUL_SUBWAY_ARRIVAL_API_KEY}
      SEOUL_TRANSIT_API_KEY: ${SEOUL_TRANSIT_API_KEY}
    volumes:
      - ./app.py:/app/app.py       # 로컬 app.py → 컨테이너 /app/app.py
    working_dir: /app
    ports:
      - "5000:5000"               # Flask API 외부 노출
    command: python3 app.py
    restart: unless-stopped


networks:
  default:
    driver: bridge
